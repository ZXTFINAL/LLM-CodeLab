{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载文本到向量数据库（chroma db）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name embeddings/text2vec-large-chinese. Creating a new one with MEAN pooling.\n",
      "Created a chunk of size 265, which is longer than the specified 10\n",
      "Created a chunk of size 34, which is longer than the specified 10\n",
      "Created a chunk of size 244, which is longer than the specified 10\n",
      "Created a chunk of size 149, which is longer than the specified 10\n",
      "Created a chunk of size 759, which is longer than the specified 10\n",
      "Created a chunk of size 580, which is longer than the specified 10\n",
      "Created a chunk of size 351, which is longer than the specified 10\n",
      "Created a chunk of size 743, which is longer than the specified 10\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import json\n",
    "from langchain.llms.base import LLM\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from typing import List, Optional\n",
    "from pathlib import Path\n",
    "from typing import Annotated, Union\n",
    "\n",
    "import typer\n",
    "from peft import AutoPeftModelForCausalLM, PeftModelForCausalLM\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedTokenizerFast,\n",
    ")\n",
    "\n",
    "# ModelType = Union[PreTrainedModel, PeftModelForCausalLM]\n",
    "# TokenizerType = Union[PreTrainedTokenizer, PreTrainedTokenizerFast]\n",
    "\n",
    "\n",
    "\n",
    "# def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "#     return Path(path).expanduser().resolve()\n",
    "\n",
    "\n",
    "# def load_model_and_tokenizer(model_dir: Union[str, Path]) -> tuple[ModelType, TokenizerType]:\n",
    "#     model_dir = _resolve_path(model_dir)\n",
    "#     if (model_dir / 'adapter_config.json').exists():\n",
    "#         model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#             model_dir, trust_remote_code=True, device_map='auto'\n",
    "#         )\n",
    "#         tokenizer_dir = model.peft_config['default'].base_model_name_or_path\n",
    "#     else:\n",
    "#         model = AutoModelForCausalLM.from_pretrained(\n",
    "#             model_dir, trust_remote_code=True, device_map='auto'\n",
    "#         )\n",
    "#         tokenizer_dir = model_dir\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\n",
    "#         tokenizer_dir, trust_remote_code=True\n",
    "#     )\n",
    "#     return model, tokenizer\n",
    "\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"embeddings/text2vec-large-chinese\",\n",
    "                                    model_kwargs={'device': \"cuda\"})\n",
    "texts = []\n",
    "for data in [\"database.txt\"]:\n",
    "    documents = TextLoader(data, encoding='utf8').load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=10, chunk_overlap=0)\n",
    "    texts += text_splitter.split_documents(documents)\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用langchain的接口进行查询并调用大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "base_url = \"http://127.0.0.1:8000/v1\"\n",
    "# llm = OpenAI(api_key=\"EMPTY\", base_url=base_url)\n",
    "# llm = ChatGLM3.load_model('chatglm3-6b')\n",
    "# template = \"\"\"{question}\"\"\"\n",
    "# prompt = PromptTemplate.from_template(template)\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "# model, tokenizer = load_model_and_tokenizer('outputting/checkpoint-10000/')\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.messages import AIMessage\n",
    "from langchain_community.llms.chatglm3 import ChatGLM3\n",
    "endpoint_url = \"http://127.0.0.1:8000/v1/chat/completions\"\n",
    "\n",
    "messages = [\n",
    "    AIMessage(content='''提示：\n",
    "                                   你是一个论文检索机器人，你要帮助检索论文中出现的关于各方面算法技术的记载'''),\n",
    "    AIMessage(content=\"欢迎问我任何问题。\"),\n",
    "]\n",
    "\n",
    "llm = ChatGLM3(\n",
    "    endpoint_url=endpoint_url,\n",
    "    max_tokens=1000,\n",
    "    prefix_messages=messages,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "# qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "while True:\n",
    "    query = input(\"请输入您的问题：\")\n",
    "    data = qa.run('''提示：\n",
    "                        你是一个论文检索机器人，你要帮助检索论文中出现的关于各方面算法技术的记载，\n",
    "                        尝试理解论文信息，并提供准确的查询结果\\n\n",
    "                        问题:\n",
    "                        '''+query+'''\\n\n",
    "                        回答：\n",
    "                        ''')\n",
    "    print(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 也可手动编写向量搜索和大模型查询的代码，优势是可以自定义向量查询和调用大模型的不同方式，更加灵活"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='其实也不复杂，pos为当前参数在seq_len的位置，i为当前参数在d_model的位置，将这二者的位置信息通过函数处理后加到前面的word embeddings里，赋予了每个位置独一无二的信息，而且赋予了相邻数据的相关性，从而达到了卷积神经网络和循环神经网络的考虑上下文的效果。\\n而具体实现过程中，我们既可以按这种方式来，也可以new一个Embedding层，通过输入连续自然数矩阵，获取位置编码矩阵。这个Embedding层和前面做文本编码的不同，它的shape为(seq_len,d_model)，输入的数据为连续自然数矩阵，shape为(,seq_len)，前向传播的输出即(batch,seq_len,d_model)。这样就可以自动学习上下文的位置联系，这样很合理，毕竟本身它也是一个(seq_len,d_model)的矩阵，直接咬死一个固定的函数反而显得不合理了。（不过这样训练的成本肯定也会高，收敛速度不好说）我们可以通过这个函数的固定矩阵反推这个Embedding的值，作为矩阵的初始化权重。\\n具体实现也很简单：\\nself.src_position_embedding = nn.Embedding(seq_len, d_model)\\n前向传播时：\\npositions = torch.arange(0, seq_len).expand(N, seq_len)\\nsrc_position_outs = self.src_position_embedding(positions)\\n而后就可以将其和word embeddings的输出结果相加\\nsrc_word_outs+src_position_outs\\n具体实现可以考虑添加Dropout等结构，帮助整体收敛的优化。', metadata={'source': 'database.txt'}), Document(page_content='其实也不复杂，pos为当前参数在seq_len的位置，i为当前参数在d_model的位置，将这二者的位置信息通过函数处理后加到前面的word embeddings里，赋予了每个位置独一无二的信息，而且赋予了相邻数据的相关性，从而达到了卷积神经网络和循环神经网络的考虑上下文的效果。\\n而具体实现过程中，我们既可以按这种方式来，也可以new一个Embedding层，通过输入连续自然数矩阵，获取位置编码矩阵。这个Embedding层和前面做文本编码的不同，它的shape为(seq_len,d_model)，输入的数据为连续自然数矩阵，shape为(,seq_len)，前向传播的输出即(batch,seq_len,d_model)。这样就可以自动学习上下文的位置联系，这样很合理，毕竟本身它也是一个(seq_len,d_model)的矩阵，直接咬死一个固定的函数反而显得不合理了。（不过这样训练的成本肯定也会高，收敛速度不好说）我们可以通过这个函数的固定矩阵反推这个Embedding的值，作为矩阵的初始化权重。\\n具体实现也很简单：\\nself.src_position_embedding = nn.Embedding(seq_len, d_model)\\n前向传播时：\\npositions = torch.arange(0, seq_len).expand(N, seq_len)\\nsrc_position_outs = self.src_position_embedding(positions)\\n而后就可以将其和word embeddings的输出结果相加\\nsrc_word_outs+src_position_outs\\n具体实现可以考虑添加Dropout等结构，帮助整体收敛的优化。', metadata={'source': 'database.txt'}), Document(page_content='2.Position Embeddings\\n为什么要有位置编码这种东西呢？在“非卷积即循环”的结构中，卷积神经网络通过卷积核来获取上下文的数据信息，循环神经网络可以通过循环的特性获取上文信息（通过Bi-lstm可以获得双向信息）。这两种结构其实都有自己的问题，卷积神经网络受卷积核的大小影响，感受野会受限，而如果卷积核很大全面考虑又丢失了和上下文联系的意义（其实等价于全连接了），循环神经网络这种套娃逻辑受梯度传播的影响，会有梯度的遗忘，即使提出了LSTM和GRU等带门控结构的模型，也会有对异常数据过于敏感的缺陷。和它们俩一样，Transformer也想获取上下文信息，所以它提出了位置编码的概念，给编码好的数据打上一个位置的标签，作为特征的一部分。\\n原文中提到了一个看上去很唬人的公式计算位置编码：', metadata={'source': 'database.txt'}), Document(page_content='2.Position Embeddings\\n为什么要有位置编码这种东西呢？在“非卷积即循环”的结构中，卷积神经网络通过卷积核来获取上下文的数据信息，循环神经网络可以通过循环的特性获取上文信息（通过Bi-lstm可以获得双向信息）。这两种结构其实都有自己的问题，卷积神经网络受卷积核的大小影响，感受野会受限，而如果卷积核很大全面考虑又丢失了和上下文联系的意义（其实等价于全连接了），循环神经网络这种套娃逻辑受梯度传播的影响，会有梯度的遗忘，即使提出了LSTM和GRU等带门控结构的模型，也会有对异常数据过于敏感的缺陷。和它们俩一样，Transformer也想获取上下文信息，所以它提出了位置编码的概念，给编码好的数据打上一个位置的标签，作为特征的一部分。\\n原文中提到了一个看上去很唬人的公式计算位置编码：', metadata={'source': 'database.txt'})]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m doc \u001b[39m=\u001b[39m db\u001b[39m.\u001b[39msimilarity_search(\u001b[39m'\u001b[39m\u001b[39m位置编码\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(doc)\n\u001b[0;32m----> 3\u001b[0m llm(prompt\u001b[39m=\u001b[39m\u001b[39m'''\u001b[39m\u001b[39m提示：\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39m                        你是一个论文检索机器人，你要帮助检索论文中出现的关于各方面算法技术的记载，\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39m                        通过检索到的文本内容，并提供准确的查询结果\u001b[39m\u001b[39m\\n\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m                        问题:\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39m                        \u001b[39m\u001b[39m'''\u001b[39m\u001b[39m+\u001b[39mquery\u001b[39m+\u001b[39m\u001b[39m'''\u001b[39m\u001b[39m\\n\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m                        搜索到的文本为：\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[39m                        \u001b[39m\u001b[39m'''\u001b[39m\u001b[39m+\u001b[39mdoc\u001b[39m+\u001b[39m\u001b[39m'''\u001b[39m\u001b[39m\\n\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m                        回答：\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[39m                        \u001b[39m\u001b[39m'''\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "doc = db.similarity_search('位置编码')\n",
    "print(doc)\n",
    "llm(prompt='''提示：\n",
    "                        你是一个论文检索机器人，你要帮助检索论文中出现的关于各方面算法技术的记载，\n",
    "                        通过检索到的文本内容，并提供准确的查询结果\\n\n",
    "                        问题:\n",
    "                        '''+query+'''\\n\n",
    "                        搜索到的文本为：\n",
    "                        '''+doc+'''\\n\n",
    "                        回答：\n",
    "                        ''')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
